{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshDandumahanti/Code00/blob/main/nanogpt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOTT9r3UqXHb"
      },
      "source": [
        "#1. Intro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KARDeenzNEsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1adab8f9-673e-457d-a7c9-00accda4df12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-01 04:01:24--  https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459564 (449K) [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/saved_models/sorcerers-stone.txt’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>] 448.79K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-01 04:01:24 (12.1 MB/s) - ‘/content/drive/MyDrive/saved_models/sorcerers-stone.txt’ saved [459564/459564]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt -O \"/content/drive/MyDrive/saved_models/sorcerers-stone.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keepsake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QDct8xY9nQB",
        "outputId": "ff3e9131-175a-4140-92ec-4a44059608b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keepsake\n",
            "  Downloading keepsake-0.4.2-py3-none-manylinux1_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos[grpc]>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from keepsake) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from googleapis-common-protos[grpc]>=1.0.0->keepsake) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0.0.dev0,>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from googleapis-common-protos[grpc]>=1.0.0->keepsake) (1.62.2)\n",
            "Installing collected packages: keepsake\n",
            "Successfully installed keepsake-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keepsake\n",
        "from keepsake import experiment"
      ],
      "metadata": {
        "id": "k2WbamILthX5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9I5yBdPGnT"
      },
      "source": [
        "#2. Reading and exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nbXThzYzqhS1"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/saved_models/sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW5Vh-_009x_",
        "outputId": "6f0f5d49-9ec5-4c29-d457-ca4ce0c23949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  441832\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZduYxLyqNQR",
        "outputId": "690c5788-f940-454f-8047-86cce7a601e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive,\n",
            "were proud to say that they were perfectly normal,\n",
            "thank you very much. They were the last people you’d\n",
            "expect to be involved in anything strange or\n",
            "mysterious, because they just didn’t hold with such\n",
            "nonsense.\n",
            "\n",
            "Mr. Dursley was the director of a firm called\n",
            "Grunnings, which made drills. He was a big, beefy\n",
            "man with hardly any neck, although he did have a\n",
            "very large mustache. Mrs. Dursley was thin and\n",
            "blonde and had nearly twice the usual amount of\n",
            "neck, which came in very useful as she spent so\n",
            "much of her time craning over garden fences, spying\n",
            "on the neighbors. The Dursley s had a small son\n",
            "called Dudley and in their opinion there was no finer\n",
            "boy anywhere.\n",
            "\n",
            "The Dursleys had everything they wanted, but they\n",
            "also had a secret, and their greatest fear was that\n",
            "somebody would discover it. They didn’t think they\n",
            "could bear it if anyone found out about the Potters.\n",
            "Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t\n"
          ]
        }
      ],
      "source": [
        "#let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paPGTnoZP0SV"
      },
      "source": [
        "#3. Tokenization and train/val split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTe4niX09fl"
      },
      "source": [
        "1. here basically all the characters are sorted and then assigned a number for each one of them.\n",
        "2. then each of the character is then encodeded to a number till whole dataset is converted into a string of numbers\n",
        "3. then it is broke into training and testing dataset\n",
        "4. After that a neuron is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7spV-TD0PFI",
        "outputId": "1a943e78-b3cb-4a89-d2f5-f7fb743f7e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYZ\\abcdefghijklmnopqrstuvwxyz—‘’“”•■\n",
            "83\n"
          ]
        }
      ],
      "source": [
        "#here are all the unique characters that occur in this text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9p3K8SN-dKu",
        "outputId": "133bc2b1-5d1d-4a55-f8db-fe8cfc5fb499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57, 58, 1, 69, 57, 54, 67, 54]\n",
            "hi there\n"
          ]
        }
      ],
      "source": [
        "#create a mapping from characters to integers\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder : take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp9Ua1mU_V2E",
        "outputId": "ecd886e0-054c-4604-adae-0bbcbefc68c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([441832]) torch.int64\n",
            "tensor([43, 31, 28,  1, 25, 38, 47,  1, 46, 31, 38,  1, 35, 32, 45, 28, 27,  0,\n",
            "         0, 36, 67,  9,  1, 50, 63, 53,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68,\n",
            "        61, 54, 74,  7,  1, 64, 55,  1, 63, 70, 62, 51, 54, 67,  1, 55, 64, 70,\n",
            "        67,  7,  1, 39, 67, 58, 71, 54, 69,  1, 27, 67, 58, 71, 54,  7,  0, 72,\n",
            "        54, 67, 54,  1, 65, 67, 64, 70, 53,  1, 69, 64,  1, 68, 50, 74,  1, 69,\n",
            "        57, 50, 69,  1, 69, 57, 54, 74,  1, 72, 54, 67, 54,  1, 65, 54, 67, 55,\n",
            "        54, 52, 69, 61, 74,  1, 63, 64, 67, 62, 50, 61,  7,  0, 69, 57, 50, 63,\n",
            "        60,  1, 74, 64, 70,  1, 71, 54, 67, 74,  1, 62, 70, 52, 57,  9,  1, 43,\n",
            "        57, 54, 74,  1, 72, 54, 67, 54,  1, 69, 57, 54,  1, 61, 50, 68, 69,  1,\n",
            "        65, 54, 64, 65, 61, 54,  1, 74, 64, 70, 78, 53,  0, 54, 73, 65, 54, 52,\n",
            "        69,  1, 69, 64,  1, 51, 54,  1, 58, 63, 71, 64, 61, 71, 54, 53,  1, 58,\n",
            "        63,  1, 50, 63, 74, 69, 57, 58, 63, 56,  1, 68, 69, 67, 50, 63, 56, 54,\n",
            "         1, 64, 67,  0, 62, 74, 68, 69, 54, 67, 58, 64, 70, 68,  7,  1, 51, 54,\n",
            "        52, 50, 70, 68, 54,  1, 69, 57, 54, 74,  1, 59, 70, 68, 69,  1, 53, 58,\n",
            "        53, 63, 78, 69,  1, 57, 64, 61, 53,  1, 72, 58, 69, 57,  1, 68, 70, 52,\n",
            "        57,  0, 63, 64, 63, 68, 54, 63, 68, 54,  9,  0,  0, 36, 67,  9,  1, 27,\n",
            "        70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57, 54,  1, 53, 58, 67,\n",
            "        54, 52, 69, 64, 67,  1, 64, 55,  1, 50,  1, 55, 58, 67, 62,  1, 52, 50,\n",
            "        61, 61, 54, 53,  0, 30, 67, 70, 63, 63, 58, 63, 56, 68,  7,  1, 72, 57,\n",
            "        58, 52, 57,  1, 62, 50, 53, 54,  1, 53, 67, 58, 61, 61, 68,  9,  1, 31,\n",
            "        54,  1, 72, 50, 68,  1, 50,  1, 51, 58, 56,  7,  1, 51, 54, 54, 55, 74,\n",
            "         0, 62, 50, 63,  1, 72, 58, 69, 57,  1, 57, 50, 67, 53, 61, 74,  1, 50,\n",
            "        63, 74,  1, 63, 54, 52, 60,  7,  1, 50, 61, 69, 57, 64, 70, 56, 57,  1,\n",
            "        57, 54,  1, 53, 58, 53,  1, 57, 50, 71, 54,  1, 50,  0, 71, 54, 67, 74,\n",
            "         1, 61, 50, 67, 56, 54,  1, 62, 70, 68, 69, 50, 52, 57, 54,  9,  1, 36,\n",
            "        67, 68,  9,  1, 27, 70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57,\n",
            "        58, 63,  1, 50, 63, 53,  0, 51, 61, 64, 63, 53, 54,  1, 50, 63, 53,  1,\n",
            "        57, 50, 53,  1, 63, 54, 50, 67, 61, 74,  1, 69, 72, 58, 52, 54,  1, 69,\n",
            "        57, 54,  1, 70, 68, 70, 50, 61,  1, 50, 62, 64, 70, 63, 69,  1, 64, 55,\n",
            "         0, 63, 54, 52, 60,  7,  1, 72, 57, 58, 52, 57,  1, 52, 50, 62, 54,  1,\n",
            "        58, 63,  1, 71, 54, 67, 74,  1, 70, 68, 54, 55, 70, 61,  1, 50, 68,  1,\n",
            "        68, 57, 54,  1, 68, 65, 54, 63, 69,  1, 68, 64,  0, 62, 70, 52, 57,  1,\n",
            "        64, 55,  1, 57, 54, 67,  1, 69, 58, 62, 54,  1, 52, 67, 50, 63, 58, 63,\n",
            "        56,  1, 64, 71, 54, 67,  1, 56, 50, 67, 53, 54, 63,  1, 55, 54, 63, 52,\n",
            "        54, 68,  7,  1, 68, 65, 74, 58, 63, 56,  0, 64, 63,  1, 69, 57, 54,  1,\n",
            "        63, 54, 58, 56, 57, 51, 64, 67, 68,  9,  1, 43, 57, 54,  1, 27, 70, 67,\n",
            "        68, 61, 54, 74,  1, 68,  1, 57, 50, 53,  1, 50,  1, 68, 62, 50, 61, 61,\n",
            "         1, 68, 64, 63,  0, 52, 50, 61, 61, 54, 53,  1, 27, 70, 53, 61, 54, 74,\n",
            "         1, 50, 63, 53,  1, 58, 63,  1, 69, 57, 54, 58, 67,  1, 64, 65, 58, 63,\n",
            "        58, 64, 63,  1, 69, 57, 54, 67, 54,  1, 72, 50, 68,  1, 63, 64,  1, 55,\n",
            "        58, 63, 54, 67,  0, 51, 64, 74,  1, 50, 63, 74, 72, 57, 54, 67, 54,  9,\n",
            "         0,  0, 43, 57, 54,  1, 27, 70, 67, 68, 61, 54, 74, 68,  1, 57, 50, 53,\n",
            "         1, 54, 71, 54, 67, 74, 69, 57, 58, 63, 56,  1, 69, 57, 54, 74,  1, 72,\n",
            "        50, 63, 69, 54, 53,  7,  1, 51, 70, 69,  1, 69, 57, 54, 74,  0, 50, 61,\n",
            "        68, 64,  1, 57, 50, 53,  1, 50,  1, 68, 54, 52, 67, 54, 69,  7,  1, 50,\n",
            "        63, 53,  1, 69, 57, 54, 58, 67,  1, 56, 67, 54, 50, 69, 54, 68, 69,  1,\n",
            "        55, 54, 50, 67,  1, 72, 50, 68,  1, 69, 57, 50, 69,  0, 68, 64, 62, 54,\n",
            "        51, 64, 53, 74,  1, 72, 64, 70, 61, 53,  1, 53, 58, 68, 52, 64, 71, 54,\n",
            "        67,  1, 58, 69,  9,  1, 43, 57, 54, 74,  1, 53, 58, 53, 63, 78, 69,  1,\n",
            "        69, 57, 58, 63, 60,  1, 69, 57, 54, 74,  0, 52, 64, 70, 61, 53,  1, 51,\n",
            "        54, 50, 67,  1, 58, 69,  1, 58, 55,  1, 50, 63, 74, 64, 63, 54,  1, 55,\n",
            "        64, 70, 63, 53,  1, 64, 70, 69,  1, 50, 51, 64, 70, 69,  1, 69, 57, 54,\n",
            "         1, 39, 64, 69, 69, 54, 67, 68,  9,  0, 36, 67, 68,  9,  1, 39, 64, 69,\n",
            "        69, 54, 67,  1, 72, 50, 68,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68, 61,\n",
            "        54, 74, 78, 68,  1, 68, 58, 68, 69, 54, 67,  7,  1, 51, 70, 69,  1, 69,\n",
            "        57, 54, 74,  1, 57, 50, 53, 63, 78, 69])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a'torch.Tensor\n",
        "import torch # we use PyTorch: http://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wtYfoMMEAeBx"
      },
      "outputs": [],
      "source": [
        "#Let's now split up the data into train and validation sets\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7V320eSA1h1",
        "outputId": "9b90f49b-99c9-4976-9b6f-e5a3a82bbc20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 31, 28,  1, 25, 38, 47,  1, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de-iNl7A-3A",
        "outputId": "15693e74-50cf-4dbe-a0df-7f2ca8e3a45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([43]) the target: 31\n",
            "when input is tensor([43, 31]) the target: 28\n",
            "when input is tensor([43, 31, 28]) the target: 1\n",
            "when input is tensor([43, 31, 28,  1]) the target: 25\n",
            "when input is tensor([43, 31, 28,  1, 25]) the target: 38\n",
            "when input is tensor([43, 31, 28,  1, 25, 38]) the target: 47\n",
            "when input is tensor([43, 31, 28,  1, 25, 38, 47]) the target: 1\n",
            "when input is tensor([43, 31, 28,  1, 25, 38, 47,  1]) the target: 46\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGQAegsZQH4e"
      },
      "source": [
        "#4. Data loader : batches of chunks of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4zovBWGBd-N",
        "outputId": "9c880bb3-408e-4bf0-878a-196eb589c1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n",
            "        [53,  1, 58, 69,  1, 64, 63, 69],\n",
            "        [ 1, 65, 64, 58, 63, 69, 68,  1],\n",
            "        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 69, 57, 54,  0, 27, 70, 67],\n",
            "        [ 1, 58, 69,  1, 64, 63, 69, 64],\n",
            "        [65, 64, 58, 63, 69, 68,  1, 55],\n",
            "        [64, 70, 69,  1, 58, 63, 69, 64]])\n",
            "-----\n",
            "when input is [55] the target: 1\n",
            "when input is [55, 1] the target: 69\n",
            "when input is [55, 1, 69] the target: 57\n",
            "when input is [55, 1, 69, 57] the target: 54\n",
            "when input is [55, 1, 69, 57, 54] the target: 0\n",
            "when input is [55, 1, 69, 57, 54, 0] the target: 27\n",
            "when input is [55, 1, 69, 57, 54, 0, 27] the target: 70\n",
            "when input is [55, 1, 69, 57, 54, 0, 27, 70] the target: 67\n",
            "when input is [53] the target: 1\n",
            "when input is [53, 1] the target: 58\n",
            "when input is [53, 1, 58] the target: 69\n",
            "when input is [53, 1, 58, 69] the target: 1\n",
            "when input is [53, 1, 58, 69, 1] the target: 64\n",
            "when input is [53, 1, 58, 69, 1, 64] the target: 63\n",
            "when input is [53, 1, 58, 69, 1, 64, 63] the target: 69\n",
            "when input is [53, 1, 58, 69, 1, 64, 63, 69] the target: 64\n",
            "when input is [1] the target: 65\n",
            "when input is [1, 65] the target: 64\n",
            "when input is [1, 65, 64] the target: 58\n",
            "when input is [1, 65, 64, 58] the target: 63\n",
            "when input is [1, 65, 64, 58, 63] the target: 69\n",
            "when input is [1, 65, 64, 58, 63, 69] the target: 68\n",
            "when input is [1, 65, 64, 58, 63, 69, 68] the target: 1\n",
            "when input is [1, 65, 64, 58, 63, 69, 68, 1] the target: 55\n",
            "when input is [1] the target: 64\n",
            "when input is [1, 64] the target: 70\n",
            "when input is [1, 64, 70] the target: 69\n",
            "when input is [1, 64, 70, 69] the target: 1\n",
            "when input is [1, 64, 70, 69, 1] the target: 58\n",
            "when input is [1, 64, 70, 69, 1, 58] the target: 63\n",
            "when input is [1, 64, 70, 69, 1, 58, 63] the target: 69\n",
            "when input is [1, 64, 70, 69, 1, 58, 63, 69] the target: 64\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i : i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_POol24Dx_F",
        "outputId": "ff5ac55c-f815-4895-8ffc-e735b2a6e47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n",
            "        [53,  1, 58, 69,  1, 64, 63, 69],\n",
            "        [ 1, 65, 64, 58, 63, 69, 68,  1],\n",
            "        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n"
          ]
        }
      ],
      "source": [
        "print(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWJ0Z1qpQllR"
      },
      "source": [
        "#5. Simplest baseline: bigram language model, loss, generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBw3OZ8D2sh",
        "outputId": "89d1e2b7-27e9-4dd1-8214-6ca4c21742ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 83])\n",
            "tensor(4.3570, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "44;5’L,OB)!,2W/()7fd Ckxae\"jR!Dtre0qUG“kB5(;QW•N\n",
            "G—oAhhv\"SsGgj4yF-H9q:Ug■jgDJ4S11Bv!6Ulpnlitw;h’()KP\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrBq6TYfhNO"
      },
      "source": [
        "#6. Training the Bigram Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pZQ1l4KnRSex"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f_7r8f9RchN",
        "outputId": "927bff5e-19fb-4c3e-8d46-ad005fc7f00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.714195728302002\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Il3MOkxh9Gu"
      },
      "source": [
        "#7. Port our code to a script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mDRWnnmRg3Q",
        "outputId": "faba828b-db5b-4d8b-da36-f9746746188a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ")T\n",
            "UT6t•-!K?n-Jqn3nIrtE\\MY,3nb849Gy'hyNb1!?ZWlElwlwp■Lgnsod‘,J\"4E/o'Z,lPA8cT‘QJ;l4GFll:qL!hztmoT!RJtKPxx:cJ!aCnm-H4Pxbnw?- k!ERFk\"y0n-(!NkWYK7hC2NY■xO75TMB;6,Z'Lz C\n",
            "egMj2pf—sOgrYW9'—M l4•12\"yF6orjlaO.”y'5RSEQ7ZLr6z2 k‘WIkPl0l(lC\n",
            "lW928?7s0a“pntsESg?5■‘n6OFaKao“UL’rLP;;n-oJjOESGsVT,JgsJl-RCh!7IJwZ\n",
            "akSn\n",
            ":xSW-J.uS”Vq)nh:e0■DZ■I’e,P)d\n",
            "w‘nGpmx, WeZW(BR)\"2Yj7vLS)TFy‘”tm N\n",
            "5AbkFj•k.,ZKy3FKH49RRMOYMs Na“p8Nx8NI3”■M?OBzK84V”tY'—ISuqGsfC■1FJHQ5W/Pz.LrKsz):Y5vor5K0nt84s”:x‘/gN3ntBR\"1qT2NwZnx!vKZE3N.w6r?f?T \n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPb9rcNVRktD"
      },
      "source": [
        "#8. The trick in self-attention: matrix multiply as weight aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcwiGHGcSLoI",
        "outputId": "9ea1b2c7-cf5e-4698-f76c-6ce69c84486c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHRcjq1NiT_M"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkPhcJsvSRff",
        "outputId": "d38f3a1b-a131-4014-f617-80052b74c199"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "T-SCPrvMSV87"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc7x_2ZOSwds"
      },
      "source": [
        "#9. V2: Using matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Evfz-ziSpCY",
        "outputId": "419b4f76-4e98-4e70-fdf4-bf9c3df62657"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCeAMYnKS7iF"
      },
      "source": [
        "#10. V3. Adding softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADZyCNR7THtR",
        "outputId": "9bb68303-76da-4829-b65b-48edc0059d6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wgr06ZLTNvP"
      },
      "source": [
        "#12. V4. self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzUOVIQ8Te8K",
        "outputId": "b0dd4679-0d24-4c4b-fe26-6c01bed39ffc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4LznjneTjur",
        "outputId": "a1221549-3064-429e-a211-385013f7aa8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evGWJ5IQTNfh"
      },
      "source": [
        "#13. n1: attention as communication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8vhwrkKT1OL"
      },
      "source": [
        "Notes:\n",
        "\n",
        "1. Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "2. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "4.In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "5. \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "6. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFVjkoXUWct"
      },
      "source": [
        "#14. Inserting a single self-attention block to our network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjkLK0DtTxvj"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiiEcX2_UrTd",
        "outputId": "ba7c8db8-5d76-4552-8906-6c05184676c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc-hkTO4VIzS",
        "outputId": "2f42f0e5-36d0-48c9-8325-11c077735ffd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68iTSVfuVP2Z",
        "outputId": "f290353e-9b9f-4104-ef6a-1010a69da6d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDSch3WwVSdU",
        "outputId": "2ffb78ec-a810-4357-fbb6-53fc335b4068"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wnWmXokVh_-",
        "outputId": "add3007b-052b-45f7-e728-94baa73f5d3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1BVAqFHVkof",
        "outputId": "73435738-bf1a-4023-bfae-b3f83f1e8e6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRL8gvEVpxY",
        "outputId": "595df959-6617-4ec5-f791-a3030f6c2812"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML5-YitWVtrZ"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKIlukc1WIu8"
      },
      "source": [
        "#15. Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsNz-qlW5EsN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VPCwf6jqWRMS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 200\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/saved_models/sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Model 2"
      ],
      "metadata": {
        "id": "IfRqbxpcGXDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUFNWtkaw8tl",
        "outputId": "4d15b608-eb36-497d-e8d0-2d40f5abbcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.212051 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xjx86Z4wyWt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W35WXSLdbasw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "908d1575-0912-452a-f327-9a319b736d7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = BigramLanguageModel()\\nm = model.to(device)\\n# print the number of parameters in the model\\nprint(sum(p.numel() for p in m.parameters())/1e6, \\'M parameters\\')\\n\\n# create a PyTorch optimizer\\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n\\nfor iter in range(max_iters):\\n\\n    # every once in a while evaluate the loss on train and val sets\\n    if iter % eval_interval == 0 or iter == max_iters - 1:\\n        losses = estimate_loss()\\n        print(f\"step {iter}: train loss {losses[\\'train\\']:.4f}, val loss {losses[\\'val\\']:.4f}\")\\n\\n    # sample a batch of data\\n    xb, yb = get_batch(\\'train\\')\\n\\n    # evaluate the loss\\n    logits, loss = model(xb, yb)\\n    optimizer.zero_grad(set_to_none=True)\\n    loss.backward()\\n    optimizer.step()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "'''\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Bfe2O7Wiw1zB",
        "outputId": "134638af-7102-493e-a93b-36d17518c046"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor iter in range(max_iters):\\n\\n    # every once in a while evaluate the loss on train and val sets\\n    if iter % eval_interval == 0 or iter == max_iters - 1:\\n        losses = estimate_loss()\\n        print(f\"step {iter}: train loss {losses[\\'train\\']:.4f}, val loss {losses[\\'val\\']:.4f}\")\\n\\n    # sample a batch of data\\n    xb, yb = get_batch(\\'train\\')\\n\\n    # evaluate the loss\\n    logits, loss = model(xb, yb)\\n    optimizer.zero_grad(set_to_none=True)\\n    loss.backward()\\n    optimizer.step()\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "'''\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4u-vRzEJR1d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import keepsake\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 200\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Initialize the experiment with hyperparameters\n",
        "experiment = keepsake.init(\n",
        "    params={\"batch_size\": batch_size, \"block_size\": block_size, \"max_iters\": max_iters, \"learning_rate\": learning_rate},\n",
        ")\n",
        "\n",
        "# Your existing code here...\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        torch.save(model, \"model.pth\")\n",
        "        # Create a checkpoint within the experiment.\n",
        "        # This saves the metrics at that point, and makes a copy of the file\n",
        "        # or directory given, which could weights and any other artifacts.\n",
        "        experiment.checkpoint(\n",
        "            path=\"model.pth\",\n",
        "            step=max_iters,\n",
        "            metrics={\"loss\": loss.item(), \"accuracy\": acc},\n",
        "            primary_metric=(\"loss\", \"minimize\"),\n",
        "        )\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Mark the end of the experiment\n",
        "experiment.stop()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "fu8XjhdwZGr_",
        "outputId": "dc629558-288f-4e8b-af29-600af4742814"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn import functional as F\\nimport keepsake\\n\\n# hyperparameters\\nbatch_size = 16\\nblock_size = 32\\nmax_iters = 200\\neval_interval = 100\\nlearning_rate = 1e-3\\ndevice = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\neval_iters = 200\\nn_embd = 64\\nn_head = 4\\nn_layer = 4\\ndropout = 0.0\\n\\ntorch.manual_seed(1337)\\n\\n# Initialize the experiment with hyperparameters\\nexperiment = keepsake.init(\\n    params={\"batch_size\": batch_size, \"block_size\": block_size, \"max_iters\": max_iters, \"learning_rate\": learning_rate},\\n)\\n\\n# Your existing code here...\\n\\nfor iter in range(max_iters):\\n    model.train()\\n\\n    # every once in a while evaluate the loss on train and val sets\\n    if iter % eval_interval == 0 or iter == max_iters - 1:\\n        losses = estimate_loss()\\n        print(f\"step {iter}: train loss {losses[\\'train\\']:.4f}, val loss {losses[\\'val\\']:.4f}\")\\n\\n        torch.save(model, \"model.pth\")\\n        # Create a checkpoint within the experiment.\\n        # This saves the metrics at that point, and makes a copy of the file\\n        # or directory given, which could weights and any other artifacts.\\n        experiment.checkpoint(\\n            path=\"model.pth\",\\n            step=max_iters,\\n            metrics={\"loss\": loss.item(), \"accuracy\": acc},\\n            primary_metric=(\"loss\", \"minimize\"),\\n        )\\n    # sample a batch of data\\n    xb, yb = get_batch(\\'train\\')\\n\\n    # evaluate the loss\\n    logits, loss = model(xb, yb)\\n    optimizer.zero_grad(set_to_none=True)\\n    loss.backward()\\n    optimizer.step()\\n\\n# Mark the end of the experiment\\nexperiment.stop()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import keepsake\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the repository path\n",
        "repository_path = \"file:///content/drive/MyDrive/saved_models/Dream5.keepsake\"\n",
        "# Create the keepsake.yaml file\n",
        "with open(\"keepsake.yaml\", \"w\") as file:\n",
        "    file.write(f\"repository: \\\"{repository_path}\\\"\")\n",
        "\n",
        "def train(learning_rate=0.1, num_epochs=25):\n",
        "    # Create an \"experiment\". This represents a run of your training script.\n",
        "    # It saves the hyperparameters you used to start the experiment.\n",
        "    experiment = keepsake.init(\n",
        "        params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\n",
        "    )\n",
        "\n",
        "    model = BigramLanguageModel()\n",
        "    m = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        model.train()\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "            # Save model checkpoint\n",
        "            torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "            # Create a checkpoint within the experiment.\n",
        "            # This saves the metrics at that point, and makes a copy of the file\n",
        "            # or directory given, which could weights and any other artifacts.\n",
        "            experiment.checkpoint(\n",
        "                path=\"model.pth\",\n",
        "                step=iter,\n",
        "                metrics={\"train_loss\": losses[\"train\"], \"val_loss\": losses[\"val\"]},\n",
        "                primary_metric=(\"loss\", \"minimize\"),\n",
        "            )\n",
        "\n",
        "        # Sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # Evaluate the loss\n",
        "        logits, loss = model(xb, yb)  # Assuming model returns logits and loss\n",
        "        logits = torch.tensor(logits)\n",
        "        yb = torch.tensor(yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "            # Plot the loss versus step graph\n",
        "        plt.plot(range(0, max_iters, eval_interval), step_losses)\n",
        "        plt.xlabel('Training Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss vs Training Step')\n",
        "        plt.show()\n",
        "\n",
        "    # Mark the end of the experiment\n",
        "    experiment.stop()\n",
        "\n",
        "train(learning_rate=0.1, num_epochs=25)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "yr9C3DGqRQU3",
        "outputId": "5e9ca82a-1af0-4890-dfdf-3d0cffb05e9a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nfrom torch import nn\\nfrom torch.autograd import Variable\\nimport keepsake\\nimport os\\nimport matplotlib.pyplot as plt\\n\\n# Specify the repository path\\nrepository_path = \"file:///content/drive/MyDrive/saved_models/Dream5.keepsake\"\\n# Create the keepsake.yaml file\\nwith open(\"keepsake.yaml\", \"w\") as file:\\n    file.write(f\"repository: \"{repository_path}\"\")\\n\\ndef train(learning_rate=0.1, num_epochs=25):\\n    # Create an \"experiment\". This represents a run of your training script.\\n    # It saves the hyperparameters you used to start the experiment.\\n    experiment = keepsake.init(\\n        params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\\n    )\\n\\n    model = BigramLanguageModel()\\n    m = model.to(device)\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    for iter in range(max_iters):\\n        model.train()\\n        if iter % eval_interval == 0 or iter == max_iters - 1:\\n            losses = estimate_loss()\\n            print(f\"step {iter}: train loss {losses[\\'train\\']:.4f}, val loss {losses[\\'val\\']:.4f}\")\\n\\n            # Save model checkpoint\\n            torch.save(model.state_dict(), \"model.pth\")\\n\\n            # Create a checkpoint within the experiment.\\n            # This saves the metrics at that point, and makes a copy of the file\\n            # or directory given, which could weights and any other artifacts.\\n            experiment.checkpoint(\\n                path=\"model.pth\",\\n                step=iter,\\n                metrics={\"train_loss\": losses[\"train\"], \"val_loss\": losses[\"val\"]},\\n                primary_metric=(\"loss\", \"minimize\"),\\n            )\\n\\n        # Sample a batch of data\\n        xb, yb = get_batch(\\'train\\')\\n\\n        # Evaluate the loss\\n        logits, loss = model(xb, yb)  # Assuming model returns logits and loss\\n        logits = torch.tensor(logits)\\n        yb = torch.tensor(yb)\\n\\n        optimizer.zero_grad(set_to_none=True)\\n        loss.backward()\\n        optimizer.step()\\n\\n            # Plot the loss versus step graph\\n        plt.plot(range(0, max_iters, eval_interval), step_losses)\\n        plt.xlabel(\\'Training Step\\')\\n        plt.ylabel(\\'Loss\\')\\n        plt.title(\\'Loss vs Training Step\\')\\n        plt.show()\\n\\n    # Mark the end of the experiment\\n    experiment.stop()\\n\\ntrain(learning_rate=0.1, num_epochs=25)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "exp = keepsake.experiments.list()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3Vhj47YASV18",
        "outputId": "a0385521-9dcb-4c91-9d1e-79d40d9bd3a9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nexp = keepsake.experiments.list()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! echo 'repository: \"file:///content/drive/MyDrive/saved_models/Dream4.keepsake\"' > keepsake.yaml\n"
      ],
      "metadata": {
        "id": "wClgMnI9QEju"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import keepsake\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 200\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Initialize the experiment with hyperparameters\n",
        "experiment = keepsake.init(\n",
        "    params={\"batch_size\": batch_size, \"block_size\": block_size, \"max_iters\": max_iters, \"learning_rate\": learning_rate},\n",
        ")\n",
        "\n",
        "# Your existing code here...\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        torch.save(model, \"model.pth\")\n",
        "        # Create a checkpoint within the experiment.\n",
        "        # This saves the metrics at that point, and makes a copy of the file\n",
        "        # or directory given, which could weights and any other artifacts.\n",
        "        experiment.checkpoint(\n",
        "            path=\"model.pth\",\n",
        "            step=iter,\n",
        "            primary_metric=(\"loss\", \"minimize\"),\n",
        "        )\n",
        "\n",
        "    # Save the training and validation losses\n",
        "    train_losses.append(losses['train'])\n",
        "    val_losses.append(losses['val'])\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Plot the training and validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
        "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the plot as an artifact within the Keepsake experiment\n",
        "experiment.log_artifact(plt, \"training_validation_loss_plot\", \"Training and Validation Loss Plot\")\n",
        "\n",
        "# Mark the end of the experiment\n",
        "experiment.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "td1fm-klPeiS",
        "outputId": "7580a8f6-9e0b-499f-b771-d6433d2d8983"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating experiment 6832cf6...\n",
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint dcfcd23, copying 'model.pth' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.3894, val loss 2.3930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 5f469fd, copying 'model.pth' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 2.3240, val loss 2.3231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint f1b114d, copying 'model.pth' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 199: train loss 2.2382, val loss 2.2496\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Experiment' object has no attribute 'log_artifact'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-cefafccd1dc8>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Save the plot as an artifact within the Keepsake experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_validation_loss_plot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training and Validation Loss Plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Mark the end of the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Experiment' object has no attribute 'log_artifact'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import keepsake\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 200\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Initialize the experiment with hyperparameters\n",
        "experiment = keepsake.init(\n",
        "    params={\"batch_size\": batch_size, \"block_size\": block_size, \"max_iters\": max_iters, \"learning_rate\": learning_rate},\n",
        ")\n",
        "\n",
        "# Your existing code here...\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        torch.save(model, \"model.pth\")\n",
        "        # Create a checkpoint within the experiment.\n",
        "        # This saves the metrics at that point, and makes a copy of the file\n",
        "        # or directory given, which could weights and any other artifacts.\n",
        "        experiment.checkpoint(\n",
        "            path=\"model.pth\",\n",
        "            step=iter,\n",
        "            primary_metric=(\"loss\", \"minimize\"),\n",
        "        )\n",
        "\n",
        "    # Save the training and validation losses\n",
        "    train_losses.append(losses['train'])\n",
        "    val_losses.append(losses['val'])\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Plot the training and validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
        "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig('training_validation_loss_plot.png')\n",
        "\n",
        "# Log the plot as an artifact within the Keepsake experiment\n",
        "experiment.checkpoint(\n",
        "    path='training_validation_loss_plot.png',\n",
        "    step=max_iters,\n",
        "    primary_metric=(\"loss\", \"minimize\"),\n",
        ")\n",
        "\n",
        "# Mark the end of the experiment\n",
        "experiment.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "XQTVl8knWDph",
        "outputId": "5dca078e-2b0b-44c5-885f-a1c918fc6275"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating experiment d8f57d8...\n",
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint bac9626, copying 'model.pth' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.1206, val loss 2.1365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 0443026, copying 'model.pth' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 2.1009, val loss 2.1077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 91b09f8, copying 'model.pth' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 199: train loss 2.0412, val loss 2.0538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 1ab1a5b, copying 'training_validation_loss_plot.png' to 'file:///content/drive/MyDrive/saved_models/Dream4.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! echo 'repository: \"file://.keepsake\"' > keepsake.yaml"
      ],
      "metadata": {
        "id": "9trnwoMdZ1kH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import keepsake\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 800\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Initialize the experiment with hyperparameters\n",
        "experiment = keepsake.init(\n",
        "    params={\"batch_size\": batch_size, \"block_size\": block_size, \"max_iters\": max_iters, \"learning_rate\": learning_rate},\n",
        ")\n",
        "\n",
        "# Your existing code here...\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    model.train()\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        torch.save(model, \"model.pth\")\n",
        "        # Create a checkpoint within the experiment.\n",
        "        # This saves the metrics at that point, and makes a copy of the file\n",
        "        # or directory given, which could weights and any other artifacts.\n",
        "        experiment.checkpoint(\n",
        "            path=\"model.pth\",\n",
        "            step=iter,\n",
        "            primary_metric=(\"loss\", \"minimize\"),\n",
        "        )\n",
        "\n",
        "    # Save the training and validation losses\n",
        "    train_losses.append(losses['train'])\n",
        "    val_losses.append(losses['val'])\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Plot the training and validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
        "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig('training_validation_loss_plot.png')\n",
        "\n",
        "# Log the plot as an artifact within the Keepsake experiment\n",
        "experiment.checkpoint(\n",
        "    path='training_validation_loss_plot.png',\n",
        "    step=max_iters,\n",
        "    primary_metric=(\"loss\", \"minimize\"),\n",
        ")\n",
        "\n",
        "# Mark the end of the experiment\n",
        "experiment.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "JFa1QQEdZ7DE",
        "outputId": "f3d8bc93-c641-46a7-c519-a39682487b20"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating experiment 9f747f8...\n",
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 0b7ade2, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 1.9991, val loss 2.0156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 8cb95ef, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train loss 1.9960, val loss 2.0066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 271fee1, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 1.9686, val loss 1.9815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 6099ab0, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: train loss 1.9278, val loss 1.9445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint b412258, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 1.8935, val loss 1.9114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint f42cde7, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: train loss 1.8684, val loss 1.8772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 6f41b0e, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 1.8478, val loss 1.8548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 0cadabd, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: train loss 1.8169, val loss 1.8453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint dfa7420, copying 'model.pth' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 799: train loss 1.8194, val loss 1.8334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m═══╡ \u001b[0mCreating checkpoint 1a890f5, copying 'training_validation_loss_plot.png' to 'file:///content/.keepsake' in the background...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import keepsake\n",
        "\n",
        "def train(learning_rate=0.1, num_epochs=25):\n",
        "    # Create an \"experiment\". This represents a run of your training script.\n",
        "    # It saves the hyperparameters you used to start the experiment.\n",
        "    experiment = keepsake.init(\n",
        "        params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\n",
        "    )\n",
        "\n",
        "    model = BigramLanguageModel()\n",
        "    m = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        model.train()\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "            torch.save(model, \"model.pth\")\n",
        "            # Create a checkpoint within the experiment.\n",
        "            # This saves the metrics at that point, and makes a copy of the file\n",
        "            # or directory given, which could weights and any other artifacts.\n",
        "            experiment.checkpoint(\n",
        "                path=\"model.pth\",\n",
        "                step=max_iters,\n",
        "                metrics={\"loss\": loss.item(), \"accuracy\": acc},\n",
        "                primary_metric=(\"loss\", \"minimize\"),\n",
        "            )\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Mark the end of the experiment\n",
        "    experiment.stop()\n",
        "\n",
        "train(learning_rate=0.1, num_epochs=25)\n",
        "'''"
      ],
      "metadata": {
        "id": "VaRbRQvnH7H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import keepsake\n",
        "\n",
        "def train(learning_rate=0.1, num_epochs=25):\n",
        "    # Create an \"experiment\". This represents a run of your training script.\n",
        "    # It saves the hyperparameters you used to start the experiment.\n",
        "    experiment = keepsake.init(\n",
        "        params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\n",
        "    )\n",
        "\n",
        "    model = BigramLanguageModel()\n",
        "    m = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        model.train()\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "            # Save model checkpoint\n",
        "            torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "            # Create a checkpoint within the experiment.\n",
        "            # This saves the metrics at that point, and makes a copy of the file\n",
        "            # or directory given, which could weights and any other artifacts.\n",
        "            experiment.checkpoint(\n",
        "                path=\"model.pth\",\n",
        "                step=iter,\n",
        "                metrics={\"train_loss\": losses[\"train\"], \"val_loss\": losses[\"val\"]},\n",
        "                primary_metric=(\"loss\", \"minimize\"),\n",
        "            )\n",
        "\n",
        "        # Sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # Evaluate the loss\n",
        "        logits, loss = model(xb, yb)  # Assuming model returns logits and loss\n",
        "        logits = torch.tensor(logits)\n",
        "        yb = torch.tensor(yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Mark the end of the experiment\n",
        "    experiment.stop()\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "8PoLKs99LD-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-remOGIAsCYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import keepsake\n",
        "\n",
        "def train(learning_rate, num_epochs):\n",
        "    # Initialize the experiment with hyperparameters\n",
        "    experiment = keepsake.init(\n",
        "        params={\"batch_size\": batch_size, \"block_size\": block_size, \"max_iters\": max_iters, \"learning_rate\": learning_rate},\n",
        "    )\n",
        "\n",
        "    # Your existing code here...\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        model.train()\n",
        "\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Mark the end of the experiment\n",
        "    experiment.stop()\n",
        "\n",
        "# Your hyperparameters and initialization code\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 600\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "train(learning_rate, num_epochs)\n",
        "'''"
      ],
      "metadata": {
        "id": "m4hjfZtUcJLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NT2NYuSpeL0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import keepsake\n",
        "\n",
        "def train(learning_rate=0.1, num_epochs=25):\n",
        "    # Create an \"experiment\". This represents a run of your training script.\n",
        "    # It saves the hyperparameters you used to start the experiment.\n",
        "    experiment = keepsake.init(\n",
        "        params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\n",
        ")\n",
        "\n",
        "\n",
        "    # Your existing setup code here...\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if epoch % eval_interval == 0 or epoch == num_epochs - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Checkpoint the model and metrics\n",
        "\n",
        "        experiment.checkpoint(\n",
        "            path=\"/content/drive/MyDrive/saved_models/johnny1.keepsake\",\n",
        "            step=epoch,\n",
        "            metrics={\"loss\": loss.item()},\n",
        "            primary_metric=(\"loss\", \"minimize\"),\n",
        "        )\n",
        "\n",
        "    # Mark experiment as stopped\n",
        "    experiment.stop()\n",
        "# Run the training function\n",
        "train(learning_rate=1e-3, num_epochs=200)\n",
        "'''"
      ],
      "metadata": {
        "id": "GtCIeZ0PdhHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo 'repository: \"file:///content/drive/MyDrive/saved_models/Dream3.keepsake\"' > keepsake.yaml\n"
      ],
      "metadata": {
        "id": "Q0DAQIMlw7Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(learning_rate=0.1, num_epochs=25)"
      ],
      "metadata": {
        "id": "2rnIpspeNFzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keepsake\n",
        "from keepsake import experiments\n",
        "\n",
        "experiments = keepsake.experiments.list()\n",
        "for experiment in experiments:\n",
        "    print(experiment)\n"
      ],
      "metadata": {
        "id": "d2mzReDYaizO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = keepsake.experiments.list()[0]\n",
        "\n",
        "# or, if you want to get a particular experiment by its ID:\n",
        "# experiment = keepsake.experiments.get(\"fb7202b\")\n",
        "\n",
        "experiment"
      ],
      "metadata": {
        "id": "N2M-6Tk8aoS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.params"
      ],
      "metadata": {
        "id": "q4ykroIaaxdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.checkpoints[1].metrics"
      ],
      "metadata": {
        "id": "zXnYr9kVazJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiments = keepsake.experiments.list()\n",
        "experiments.plot()"
      ],
      "metadata": {
        "id": "SYfEXxs2a8OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import keepsake\n",
        "\n",
        "def train(learning_rate, num_epochs):\n",
        "    # Save training code and hyperparameters\n",
        "    experiment = keepsake.init(\n",
        "        repository=\"/content/drive/MyDrive/saved_models\",\n",
        "        directory = \"keepsake.yaml\",\n",
        "        params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\n",
        "    )\n",
        "\n",
        "    # Initialize your model\n",
        "    model = Model()\n",
        "    model = model.to(device)  # Assuming 'device' is defined\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ... train step ...\n",
        "\n",
        "        # Save model weights and metrics\n",
        "        torch.save(model.state_dict(), f\"model_{epoch}.pth\")\n",
        "        experiment.checkpoint(\n",
        "            path=f\"model_{epoch}.pth\",\n",
        "            metrics={\"loss\": loss, \"accuracy\": accuracy}\n",
        "        )\n",
        "'''"
      ],
      "metadata": {
        "id": "mlXXbj0Fy0mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Generate some dummy data\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(2, size=(1000, 1))\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=100, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the checkpoint callback\n",
        "checkpoint = ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Train the model\n",
        "model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
        "'''"
      ],
      "metadata": {
        "id": "drGguNb1NuO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import keepsake\n",
        "\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Save training code and hyperparameters\n",
        "experiment = keepsake.init(\n",
        "    directory=\"/content/drive/MyDrive/saved_models/\",\n",
        "    params={\"learning_rate\": learning_rate, \"num_epochs\": num_epochs},\n",
        ")\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Log metrics\n",
        "        experiment.keep_metrics({\"train_loss\": losses['train'], \"val_loss\": losses['val']}, step=iter)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save model weights\n",
        "    experiment.keep_checkpoint(model.state_dict(), step=iter)\n",
        "\n",
        "# Save final model weights\n",
        "experiment.keep_checkpoint(model.state_dict(), step=max_iters, name=\"final_model\")\n",
        "'''"
      ],
      "metadata": {
        "id": "KM5b1AwX8U1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hsc6pTSr7aUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "caj-0qDQ7aQf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0327NQqj_l"
      },
      "source": [
        "#16. Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3H0DtRmq86k"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/saved_models/modelboy_l6.h5')\n",
        "# Save model weights and metrics\n",
        "experiment.checkpoint(\n",
        "    path=\"/content/drive/MyDrive/NanoGPT Trials/Layer 2 New/modelboy_l6.h5\",\n",
        "          metrics={\"loss\": loss, \"accuracy\": accuracy}\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NuzjhaOaSUC"
      },
      "source": [
        "#17. Prompting and output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7VjKMqLrWWC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJryUhL54Os9",
        "outputId": "5c0e661f-0cda-4e8d-b81e-c65efab10cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is the difference between harry and hagrid twin over and broth\n",
            "drofe. Throwing, thought of the troathed\n",
            "on the growies pier and heads, shared vitia a sup) ”\n",
            "gothe. Flaming!”\n",
            "\n",
            "“I’ll goin od lood jon, I tried eer Isna had varching and riblin could be.\n",
            "he threw and squaaatly from pointing himself than rest\n",
            "broom. Throw, but Evera Thous.\n",
            "\n",
            "“Well read onto easten bice kack\n",
            "and brhing the corridors.\n",
            "\n",
            "“I’m seed his going to first,” buars appoathed.\n",
            "\n",
            "Anyther of threw, and Dout it? You not,”\n",
            "beasle Harry tragain.”\n",
            "\n",
            "“No, Peoll’s ofect of ‘necurious a\n",
            "bump it ontention vooiced had an enough a plate hoot\n",
            "class light back. “And was roop coldwing\n",
            "broomsticks why hard gold, “ wream to an oving a such Gringer. Now were whole started narrow chocless, at\n",
            "those innoeth!” he said?”\n",
            "\n",
            "But he or starry, seaving, looking with seen. Wwasn’t\n",
            "do you want to be about a laugh\n",
            "boy ploce hard out up and under down\n",
            "Hagrid that bosning?” Harry rosed piled corridor. “Professor Malkon, doces sharp stlyether later to de.\n",
            "\n",
            "“Thousand hool.” Snape athat Hermione trought tried to the\n",
            "shots, wasn’t back, I tin Malte in they’d hoter has though .. .. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "“I’ll even line. I met out. Longboct .. I\n",
            "warth.\n",
            "\n",
            "“Not we to do of a day offaint about the wrong togrived away! Che Phothin, though three,\n",
            "\n",
            "“Liook atch, onother was a botic, it’s an atter,\n",
            "and tarter Leet; a’ll dill antick both wollow. Who you seven hold go — that\n",
            "seach Harry’s drosned try teached out of happen, about yously at\n",
            "with Dumbledore’s owled to do keep and what’s made\n",
            "carefirasts. They atiman a talk and bungling horrold our book, that\n",
            "the Snotatie. “Wet not Evermy Twowcard, Hermbustardly.”med Ron and a\n",
            "home Like ter, ithinger had mutth his\n",
            "wabe storted in thair and classly toos\n",
            "dickly door.\n",
            "\n",
            "“Quid’s usual exactly bacos of trying\n",
            "eages bave was so pun.\n",
            "\n",
            "“Got,” said Snape bound to stooth, who all tran. I say and teashe, wellent Harry’s watched turned to the en grow, pointing\n",
            "and apaoned upbed althone. “Wrood\n",
            "up or the wand. Hogwarts — hocnap a\n",
            "secondly, but though the swungy talking. They ha\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the model and move it to the same device as the tokens\n",
        "model = BigramLanguageModel().to(device)\n",
        "state_dict = torch.load('/content/drive/MyDrive/saved_models/modelboy_l6.h5')\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Tokenize the starting prompt\n",
        "prompt = \"what is the difference between harry and hagrid\"\n",
        "tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate the conversation\n",
        "max_tokens = 2000\n",
        "word_count = 0\n",
        "\n",
        "# Generate up to max_tokens words\n",
        "for _ in range(max_tokens):\n",
        "    # Generate the next token\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(tokens, max_new_tokens=1)\n",
        "    new_token = output[:, -1].item()\n",
        "\n",
        "    # Convert the new token to a character and append to the prompt\n",
        "    new_char = itos[new_token]\n",
        "    if new_char == '':\n",
        "      word_count += 1\n",
        "    tokens = torch.cat((tokens, torch.tensor([[new_token]], device=device)), dim=1)\n",
        "\n",
        "    if word_count >= 2000:\n",
        "      break\n",
        "\n",
        "# Convert the token indices to characters and join them into a string\n",
        "generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iahifShBUMj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ZBR5CFAuVnP_",
        "outputId": "36480ce6-b725-4716-e405-dc5fddaca41c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# List of 50 prompts\\nprompts = [\\n    \"What is the difference between Harry and Hagrid?\",\\n    \"How did Voldemort lose his powers?\",\\n    \"Why did Dumbledore trust Snape?\",\\n    \"Describe a typical day in the life of a Hogwarts student.\",\\n    \"What are some magical creatures Harry encountered during his time at Hogwarts?\",\\n    \"Explain how the Triwizard Tournament works.\",\\n    \"Describe a spell that Harry learned in Defense Against the Dark Arts class.\",\\n    \"What is the significance of Harry\\'s lightning-shaped scar?\",\\n    \"Discuss the relationship between Harry, Ron, and Hermione.\",\\n    \"Describe a Quidditch match at Hogwarts.\",\\n    \"What challenges did Harry face in the Chamber of Secrets?\",\\n    \"Explain the concept of Horcruxes and how they were used by Voldemort.\",\\n    \"Describe a lesson with Professor Snape in Potions class.\",\\n    \"Discuss the role of Dumbledore as headmaster of Hogwarts.\",\\n    \"Explain the history and significance of the Deathly Hallows.\",\\n    \"Describe a feast in the Great Hall at Hogwarts.\",\\n    \"Discuss the rivalry between Gryffindor and Slytherin houses.\",\\n    \"Describe a magical object that plays a key role in the Harry Potter series.\",\\n    \"Explain the process of brewing Polyjuice Potion.\",\\n    \"Discuss the significance of Harry\\'s Patronus.\",\\n    \"Describe a visit to Hogsmeade village.\",\\n    \"Explain the concept of Animagi and how they are registered.\",\\n    \"Discuss the role of house-elves in the Wizarding World.\",\\n    \"Describe a visit to the Forbidden Forest at Hogwarts.\",\\n    \"Explain how the Marauder\\'s Map works.\",\\n    \"Discuss the history and significance of the Mirror of Erised.\",\\n    \"Describe a magical creature from the Harry Potter series and its characteristics.\",\\n    \"Explain the rules and regulations of the Hogwarts Express train.\",\\n    \"Discuss the importance of wandlore in the Wizarding World.\",\\n    \"Describe a magical plant found in the Hogwarts greenhouse.\",\\n    \"Explain the significance of the Sorting Hat ceremony at Hogwarts.\",\\n    \"Discuss the role of the Ministry of Magic in regulating the Wizarding World.\",\\n    \"Describe a magical duel between two characters in the Harry Potter series.\",\\n    \"Explain the role of the Triwizard Cup in the Triwizard Tournament.\",\\n    \"Discuss the significance of the Yule Ball in the fourth Harry Potter book.\",\\n    \"Describe a magical object from the Harry Potter series that has the power to transport individuals.\",\\n    \"Explain the process of creating a Horcrux and why it is considered dark magic.\",\\n    \"Discuss the importance of Quidditch in the Wizarding World.\",\\n    \"Describe a magical creature that is considered dangerous in the Harry Potter series.\",\\n    \"Explain the concept of Occlumency and why it is important for wizards to learn.\",\\n    \"Discuss the role of the Hogwarts Express train in transporting students to Hogwarts.\",\\n    \"Describe a magical artifact that plays a key role in one of the Harry Potter books.\",\\n    \"Explain the concept of the Triwizard Tournament and how it is organized.\",\\n    \"Discuss the importance of the Room of Requirement in the Harry Potter series.\",\\n    \"Describe a magical spell that is used for defensive purposes in the Wizarding World.\",\\n    \"Explain the significance of the prophecy regarding Harry and Voldemort.\",\\n    \"Discuss the role of house points in determining the House Cup winner at Hogwarts.\",\\n    \"Describe a magical creature that is considered to be mythical in the Harry Potter series.\",\\n    \"Explain the concept of the Unbreakable Vow and its consequences.\",\\n    \"Discuss the role of the Hogwarts ghosts in the Harry Potter series.\",\\n    \"Describe a magical object that has the power to manipulate time.\",\\n    \"Explain the significance of Harry\\'s Invisibility Cloak in the Harry Potter series.\"\\n]\\n\\n# Move the entire model to the same device as the tokens\\nmodel.to(device)\\n\\n# Generate text for each prompt\\nfor i, prompt in enumerate(prompts):\\n    print(f\"Generating text for prompt {i + 1}\")\\n\\n    # Tokenize the starting prompt\\n    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\\n    max_tokens = 2000\\n    word_count = 0\\n\\n    # Generate up to max_tokens words\\n    for _ in range(max_tokens):\\n        with torch.no_grad():\\n            output = model.generate(tokens, max_new_tokens=1)\\n        new_token = output[:, -1].item()\\n        new_char = itos[new_token]\\n        if new_char == \\'\\':\\n            word_count += 1\\n        new_tensor = torch.tensor([[new_token]], device=device)\\n        tokens = torch.cat((tokens, new_tensor), dim=1)\\n\\n        if word_count >= 15000:\\n            break\\n\\n    # Convert the token indices to characters and join them into a string\\n    generated_text = \\'\\'.join([itos[idx] for idx in tokens.squeeze().tolist()])\\n    print(generated_text)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "'''\n",
        "# List of 50 prompts\n",
        "prompts = [\n",
        "    \"What is the difference between Harry and Hagrid?\",\n",
        "    \"How did Voldemort lose his powers?\",\n",
        "    \"Why did Dumbledore trust Snape?\",\n",
        "    \"Describe a typical day in the life of a Hogwarts student.\",\n",
        "    \"What are some magical creatures Harry encountered during his time at Hogwarts?\",\n",
        "    \"Explain how the Triwizard Tournament works.\",\n",
        "    \"Describe a spell that Harry learned in Defense Against the Dark Arts class.\",\n",
        "    \"What is the significance of Harry's lightning-shaped scar?\",\n",
        "    \"Discuss the relationship between Harry, Ron, and Hermione.\",\n",
        "    \"Describe a Quidditch match at Hogwarts.\",\n",
        "    \"What challenges did Harry face in the Chamber of Secrets?\",\n",
        "    \"Explain the concept of Horcruxes and how they were used by Voldemort.\",\n",
        "    \"Describe a lesson with Professor Snape in Potions class.\",\n",
        "    \"Discuss the role of Dumbledore as headmaster of Hogwarts.\",\n",
        "    \"Explain the history and significance of the Deathly Hallows.\",\n",
        "    \"Describe a feast in the Great Hall at Hogwarts.\",\n",
        "    \"Discuss the rivalry between Gryffindor and Slytherin houses.\",\n",
        "    \"Describe a magical object that plays a key role in the Harry Potter series.\",\n",
        "    \"Explain the process of brewing Polyjuice Potion.\",\n",
        "    \"Discuss the significance of Harry's Patronus.\",\n",
        "    \"Describe a visit to Hogsmeade village.\",\n",
        "    \"Explain the concept of Animagi and how they are registered.\",\n",
        "    \"Discuss the role of house-elves in the Wizarding World.\",\n",
        "    \"Describe a visit to the Forbidden Forest at Hogwarts.\",\n",
        "    \"Explain how the Marauder's Map works.\",\n",
        "    \"Discuss the history and significance of the Mirror of Erised.\",\n",
        "    \"Describe a magical creature from the Harry Potter series and its characteristics.\",\n",
        "    \"Explain the rules and regulations of the Hogwarts Express train.\",\n",
        "    \"Discuss the importance of wandlore in the Wizarding World.\",\n",
        "    \"Describe a magical plant found in the Hogwarts greenhouse.\",\n",
        "    \"Explain the significance of the Sorting Hat ceremony at Hogwarts.\",\n",
        "    \"Discuss the role of the Ministry of Magic in regulating the Wizarding World.\",\n",
        "    \"Describe a magical duel between two characters in the Harry Potter series.\",\n",
        "    \"Explain the role of the Triwizard Cup in the Triwizard Tournament.\",\n",
        "    \"Discuss the significance of the Yule Ball in the fourth Harry Potter book.\",\n",
        "    \"Describe a magical object from the Harry Potter series that has the power to transport individuals.\",\n",
        "    \"Explain the process of creating a Horcrux and why it is considered dark magic.\",\n",
        "    \"Discuss the importance of Quidditch in the Wizarding World.\",\n",
        "    \"Describe a magical creature that is considered dangerous in the Harry Potter series.\",\n",
        "    \"Explain the concept of Occlumency and why it is important for wizards to learn.\",\n",
        "    \"Discuss the role of the Hogwarts Express train in transporting students to Hogwarts.\",\n",
        "    \"Describe a magical artifact that plays a key role in one of the Harry Potter books.\",\n",
        "    \"Explain the concept of the Triwizard Tournament and how it is organized.\",\n",
        "    \"Discuss the importance of the Room of Requirement in the Harry Potter series.\",\n",
        "    \"Describe a magical spell that is used for defensive purposes in the Wizarding World.\",\n",
        "    \"Explain the significance of the prophecy regarding Harry and Voldemort.\",\n",
        "    \"Discuss the role of house points in determining the House Cup winner at Hogwarts.\",\n",
        "    \"Describe a magical creature that is considered to be mythical in the Harry Potter series.\",\n",
        "    \"Explain the concept of the Unbreakable Vow and its consequences.\",\n",
        "    \"Discuss the role of the Hogwarts ghosts in the Harry Potter series.\",\n",
        "    \"Describe a magical object that has the power to manipulate time.\",\n",
        "    \"Explain the significance of Harry's Invisibility Cloak in the Harry Potter series.\"\n",
        "]\n",
        "\n",
        "# Move the entire model to the same device as the tokens\n",
        "model.to(device)\n",
        "\n",
        "# Generate text for each prompt\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating text for prompt {i + 1}\")\n",
        "\n",
        "    # Tokenize the starting prompt\n",
        "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    max_tokens = 2000\n",
        "    word_count = 0\n",
        "\n",
        "    # Generate up to max_tokens words\n",
        "    for _ in range(max_tokens):\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(tokens, max_new_tokens=1)\n",
        "        new_token = output[:, -1].item()\n",
        "        new_char = itos[new_token]\n",
        "        if new_char == '':\n",
        "            word_count += 1\n",
        "        new_tensor = torch.tensor([[new_token]], device=device)\n",
        "        tokens = torch.cat((tokens, new_tensor), dim=1)\n",
        "\n",
        "        if word_count >= 15000:\n",
        "            break\n",
        "\n",
        "    # Convert the token indices to characters and join them into a string\n",
        "    generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "    print(generated_text)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "nulzoqIDZed2",
        "outputId": "6ffeb8f7-1136-4ac3-d781-c14a05244ef7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# List of 50 prompts\\nprompts = [\\n    \"What is the difference between Harry and Hagrid?\",\\n    \"How did Voldemort lose his powers?\",\\n    \"Why did Dumbledore trust Snape?\",\\n    # Add your remaining prompts here\\n]\\n\\n# Move the entire model to the same device as the tokens\\nmodel.to(device)\\n\\n# Generate the conversation for each prompt\\nfor i, prompt in enumerate(prompts):\\n    print(f\"Generating text for prompt {i + 1}\")\\n    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\\n    max_tokens = 2000\\n    word_count = 0\\n    while True:\\n        with torch.no_grad():\\n            output = model.generate(tokens, max_new_tokens=1)\\n        new_token = output[:, -1].item()\\n        new_char = itos[new_token]\\n        if new_char == \\'\\':\\n            word_count += 1\\n        new_tensor = torch.tensor([[new_token]], device=device)\\n        tokens = torch.cat((tokens, new_tensor), dim=1)\\n\\n        if word_count >= 2000:\\n            break\\n\\n    generated_text = \\'\\'.join([itos[idx] for idx in tokens.squeeze().tolist()])\\n    print(generated_text)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "'''\n",
        "# List of 50 prompts\n",
        "prompts = [\n",
        "    \"What is the difference between Harry and Hagrid?\",\n",
        "    \"How did Voldemort lose his powers?\",\n",
        "    \"Why did Dumbledore trust Snape?\",\n",
        "    # Add your remaining prompts here\n",
        "]\n",
        "\n",
        "# Move the entire model to the same device as the tokens\n",
        "model.to(device)\n",
        "\n",
        "# Generate the conversation for each prompt\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating text for prompt {i + 1}\")\n",
        "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    max_tokens = 2000\n",
        "    word_count = 0\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(tokens, max_new_tokens=1)\n",
        "        new_token = output[:, -1].item()\n",
        "        new_char = itos[new_token]\n",
        "        if new_char == '':\n",
        "            word_count += 1\n",
        "        new_tensor = torch.tensor([[new_token]], device=device)\n",
        "        tokens = torch.cat((tokens, new_tensor), dim=1)\n",
        "\n",
        "        if word_count >= 2000:\n",
        "            break\n",
        "\n",
        "    generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "    print(generated_text)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1C7VquWlbZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96f5e27-4896-487b-b042-0e4937781cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryCd8P26lmHL",
        "outputId": "ddd8c777-db7d-4155-8044-5308efea9a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text saved to file.\n"
          ]
        }
      ],
      "source": [
        "# Path to save the file\n",
        "file_path = '/content/drive/My Drive/saved_models/generated_text_l06.txt'\n",
        "\n",
        "# Write text to file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(generated_text)\n",
        "\n",
        "print(\"Text saved to file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqxoKXLJwx2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK9RtdG0yMvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjN3JTdnyQJN"
      },
      "source": [
        "#17. Analysing Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6WL0cK4yU8o",
        "outputId": "df57156c-816d-4c51-9725-13dc322b82b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matching words: 144\n",
            "Accuracy: 0.5647058823529412\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Function to tokenize text into words\n",
        "def tokenize(text):\n",
        "    return set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "\n",
        "# Load the contents of both files\n",
        "file1_path = '/content/drive/MyDrive/saved_models/sorcerers-stone.txt'\n",
        "file2_path = '/content/drive/MyDrive/saved_models/generated_text_l06.txt'\n",
        "\n",
        "with open(file1_path, 'r', encoding='utf-8') as file1:\n",
        "    text1 = file1.read()\n",
        "\n",
        "with open(file2_path, 'r', encoding='utf-8') as file2:\n",
        "    text2 = file2.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words_file1 = tokenize(text1)\n",
        "words_file2 = tokenize(text2)\n",
        "\n",
        "# Calculate the number of words in file 2 that are also in file 1\n",
        "matching_words = len(words_file2.intersection(words_file1))\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = matching_words / len(words_file2) if len(words_file2) > 0 else 0\n",
        "\n",
        "print(f\"Number of matching words: {matching_words}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the dictionaries to files\n",
        "dict1_path = '/content/drive/MyDrive/saved_models/dict3.json'\n",
        "dict2_path = '/content/drive/MyDrive/saved_models/dict4.json'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFuOz8GE-AAO"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "dA9I5yBdPGnT",
        "paPGTnoZP0SV",
        "CGQAegsZQH4e",
        "EWJ0Z1qpQllR",
        "2HrBq6TYfhNO",
        "9Il3MOkxh9Gu",
        "tPb9rcNVRktD",
        "zc7x_2ZOSwds",
        "HCeAMYnKS7iF",
        "7Wgr06ZLTNvP",
        "evGWJ5IQTNfh",
        "5kFVjkoXUWct"
      ],
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1-8d6hucTibNQrLvxmQFiaRZ8ek9N9k4n",
      "authorship_tag": "ABX9TyMptOWIoGPWE2KChv36Ct1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}